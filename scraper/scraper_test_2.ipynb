{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7825e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install python-dotenv\n",
    "# !{sys.executable} -m pip install google-api-python-client\n",
    "# !{sys.executable} -m pip install iteration-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daba4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Link - https://github.com/onlyphantom/youtube_api_python/blob/main/yt_public.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65cf3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from iteration_utilities import unique_everseen\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "396873f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "426586a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments(response_items, csv_output=False):\n",
    "\n",
    "    for res in response_items:\n",
    "\n",
    "        # loop through the replies\n",
    "        if 'replies' in res.keys():\n",
    "            for reply in res['replies']['comments']:\n",
    "                comment = reply['snippet']\n",
    "                comment['commentId'] = reply['id']\n",
    "                comments.append(comment)\n",
    "        else:\n",
    "            comment = {}\n",
    "            comment['snippet'] = res['snippet']['topLevelComment']['snippet']\n",
    "            comment['snippet']['parentId'] = None\n",
    "            comment['snippet']['commentId'] = res['snippet']['topLevelComment']['id']\n",
    "\n",
    "            comments.append(comment['snippet'])\n",
    "\n",
    "    if csv_output:\n",
    "         make_csv(comments)\n",
    "    \n",
    "#     print(f'Finished processing {len(comments)} comments.')\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d73974b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(comments, channelID=None):\n",
    "    header = comments[0].keys()\n",
    "\n",
    "    if channelID:\n",
    "#         filename = f'comments_{channelID}_{today}.csv'\n",
    "        filename = f'comments_july6.csv'\n",
    "    else:\n",
    "#         filename = f'comments_{today}.csv'\n",
    "        filename = f'comments_july6.csv'\n",
    "\n",
    "    with open(filename, 'a', encoding='utf8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23ec1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c106bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a service object for interacting with the YouTube Data API\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY, cache_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5bb12182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_threads(videoID, to_csv=False):\n",
    "    \n",
    "    comments_list = []\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part='id,replies,snippet',\n",
    "        videoId=videoID,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    # if there is nextPageToken, then keep calling the API\n",
    "    while response.get('nextPageToken', None):\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='id,replies,snippet',\n",
    "            videoId=videoID,\n",
    "            pageToken=response['nextPageToken']\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    comments_list = list(unique_everseen(comments_list))\n",
    "\n",
    "    print(f\"Finished fetching comments for {videoID}. {len(comments_list)} comments found.\")\n",
    "    \n",
    "    if to_csv:\n",
    "#         make_csv(comments_list, videoID)\n",
    "        make_csv(comments_list)\n",
    "    return comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b50d3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = [\n",
    "    \"HsGa76se5Ns\",\n",
    "    \"saJ7uiVwtEI\",\n",
    "    \"QsW-ygDmhK8\",\n",
    "    \"L_AVwEDNr70\",\n",
    "    \"XTVOPH8AHEM\",\n",
    "    \"LJ2U78nOi7c\",\n",
    "    \"ZmblPQ1XR7k\",\n",
    "    \"A6-yGHgrst4\",\n",
    "    \"KfyYF5_pDzY\",\n",
    "    \"xKSwVOzsOFk\",\n",
    "    \"EglsC8Xix8s\",\n",
    "    \"s8vtfwT2GW8\",\n",
    "    \"txR94UXx-5U\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7114b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:   8%|████▉                                                           | 1/13 [00:00<00:09,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for HsGa76se5Ns. 43 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  15%|█████████▊                                                      | 2/13 [00:01<00:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for saJ7uiVwtEI. 97 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  23%|██████████████▊                                                 | 3/13 [00:02<00:06,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for QsW-ygDmhK8. 102 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  31%|███████████████████▋                                            | 4/13 [00:02<00:04,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for L_AVwEDNr70. 103 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  38%|████████████████████████▌                                       | 5/13 [00:03<00:04,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for XTVOPH8AHEM. 156 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  46%|█████████████████████████████▌                                  | 6/13 [00:04<00:06,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for LJ2U78nOi7c. 247 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  54%|██████████████████████████████████▍                             | 7/13 [00:43<01:19, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for ZmblPQ1XR7k. 2184 comments found.\n",
      "Finished fetching comments for A6-yGHgrst4. 2197 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  69%|████████████████████████████████████████████▎                   | 9/13 [00:46<00:28,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for KfyYF5_pDzY. 2249 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  77%|████████████████████████████████████████████████▍              | 10/13 [00:48<00:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for xKSwVOzsOFk. 2280 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  85%|█████████████████████████████████████████████████████▎         | 11/13 [01:03<00:16,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for EglsC8Xix8s. 2742 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  92%|██████████████████████████████████████████████████████████▏    | 12/13 [01:52<00:20, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for s8vtfwT2GW8. 3836 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|███████████████████████████████████████████████████████████████| 13/13 [02:13<00:00, 10.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for txR94UXx-5U. 4206 comments found.\n",
      "CPU times: total: 1min 7s\n",
      "Wall time: 2min 13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == '__main__':\n",
    "    # get comments\n",
    "#     response = comment_threads(videoID='7Kt6ouYqacQ', to_csv=True)\n",
    "#     print(response)\n",
    "    for video_id in tqdm(video_ids, desc='Processing videos'):\n",
    "        response = comment_threads(videoID=video_id, to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cfc31ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nfrom dotenv import load_dotenv\\nfrom googleapiclient.discovery import build\\nfrom iteration_utilities import unique_everseen\\nimport csv\\ncomments = []\\ndef make_csv(comments, channelID=None):\\n    header = comments[0].keys()\\n\\n    if channelID:\\n#         filename = f\\'comments_{channelID}_{today}.csv\\'\\n        filename = f\\'comments.csv\\'\\n    else:\\n#         filename = f\\'comments_{today}.csv\\'\\n        filename = f\\'comments.csv\\'\\n\\n    with open(filename, \\'w\\', encoding=\\'utf8\\', newline=\\'\\') as f:\\n        writer = csv.DictWriter(f, fieldnames=header, extrasaction=\\'ignore\\')\\n        writer.writeheader()\\n        writer.writerows(comments)\\ndef comment_threads(videoID, to_csv=False):\\n    \\n    comments_list = []\\n    \\n    request = youtube.commentThreads().list(\\n        part=\\'id,replies,snippet\\',\\n        videoId=videoID,\\n    )\\n    response = request.execute()\\n    comments_list.extend(process_comments(response[\\'items\\']))\\n\\n    # if there is nextPageToken, then keep calling the API\\n    while response.get(\\'nextPageToken\\', None):\\n        request = youtube.commentThreads().list(\\n            part=\\'id,replies,snippet\\',\\n            videoId=videoID,\\n            pageToken=response[\\'nextPageToken\\']\\n        )\\n        response = request.execute()\\n        comments_list.extend(process_comments(response[\\'items\\']))\\n\\n    comments_list = list(unique_everseen(comments_list))\\n\\n    print(f\"Finished fetching comments for {videoID}. {len(comments_list)} comments found.\")\\n    \\n    if to_csv:\\n#         make_csv(comments_list, videoID)\\n        make_csv(comments_list)\\n    return comments_list\\nif __name__ == \\'__main__\\':\\n    # get comments\\n    response = comment_threads(videoID=\\'G4EVRUFttG0\\', to_csv=True)\\n    print(response)\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from iteration_utilities import unique_everseen\n",
    "import csv\n",
    "comments = []\n",
    "def make_csv(comments, channelID=None):\n",
    "    header = comments[0].keys()\n",
    "\n",
    "    if channelID:\n",
    "#         filename = f'comments_{channelID}_{today}.csv'\n",
    "        filename = f'comments.csv'\n",
    "    else:\n",
    "#         filename = f'comments_{today}.csv'\n",
    "        filename = f'comments.csv'\n",
    "\n",
    "    with open(filename, 'w', encoding='utf8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comments)\n",
    "def comment_threads(videoID, to_csv=False):\n",
    "    \n",
    "    comments_list = []\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part='id,replies,snippet',\n",
    "        videoId=videoID,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    # if there is nextPageToken, then keep calling the API\n",
    "    while response.get('nextPageToken', None):\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='id,replies,snippet',\n",
    "            videoId=videoID,\n",
    "            pageToken=response['nextPageToken']\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    comments_list = list(unique_everseen(comments_list))\n",
    "\n",
    "    print(f\"Finished fetching comments for {videoID}. {len(comments_list)} comments found.\")\n",
    "    \n",
    "    if to_csv:\n",
    "#         make_csv(comments_list, videoID)\n",
    "        make_csv(comments_list)\n",
    "    return comments_list\n",
    "if __name__ == '__main__':\n",
    "    # get comments\n",
    "    response = comment_threads(videoID='G4EVRUFttG0', to_csv=True)\n",
    "    print(response)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

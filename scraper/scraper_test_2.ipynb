{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7825e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install python-dotenv\n",
    "# !{sys.executable} -m pip install google-api-python-client\n",
    "# !{sys.executable} -m pip install iteration-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daba4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Link - https://github.com/onlyphantom/youtube_api_python/blob/main/yt_public.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65cf3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from iteration_utilities import unique_everseen\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396873f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426586a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments(response_items, csv_output=False):\n",
    "\n",
    "    for res in response_items:\n",
    "\n",
    "        # loop through the replies\n",
    "        if 'replies' in res.keys():\n",
    "            for reply in res['replies']['comments']:\n",
    "                comment = reply['snippet']\n",
    "                comment['commentId'] = reply['id']\n",
    "                comments.append(comment)\n",
    "        else:\n",
    "            comment = {}\n",
    "            comment['snippet'] = res['snippet']['topLevelComment']['snippet']\n",
    "            comment['snippet']['parentId'] = None\n",
    "            comment['snippet']['commentId'] = res['snippet']['topLevelComment']['id']\n",
    "\n",
    "            comments.append(comment['snippet'])\n",
    "\n",
    "    if csv_output:\n",
    "         make_csv(comments)\n",
    "    \n",
    "#     print(f'Finished processing {len(comments)} comments.')\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73974b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(comments, channelID=None):\n",
    "    header = comments[0].keys()\n",
    "\n",
    "    if channelID:\n",
    "#         filename = f'comments_{channelID}_{today}.csv'\n",
    "        filename = f'comments_july6.csv'\n",
    "    else:\n",
    "#         filename = f'comments_{today}.csv'\n",
    "        filename = f'comments_july6.csv'\n",
    "\n",
    "    with open(filename, 'a', encoding='utf8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ec1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c106bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a service object for interacting with the YouTube Data API\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY, cache_discovery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bb12182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_threads(videoID, to_csv=False):\n",
    "    \n",
    "    comments_list = []\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part='id,replies,snippet',\n",
    "        videoId=videoID,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    # if there is nextPageToken, then keep calling the API\n",
    "    while response.get('nextPageToken', None):\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='id,replies,snippet',\n",
    "            videoId=videoID,\n",
    "            pageToken=response['nextPageToken']\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    comments_list = list(unique_everseen(comments_list))\n",
    "\n",
    "    print(f\"Finished fetching comments for {videoID}. {len(comments_list)} comments found.\")\n",
    "    \n",
    "    if to_csv:\n",
    "#         make_csv(comments_list, videoID)\n",
    "        make_csv(comments_list)\n",
    "    return comments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50d3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = [\n",
    "    'jV78iR4Vi68',\n",
    "    'U62U2cgPagI',\n",
    "    'TB7yAjRgO-c',\n",
    "    'BTUQ8dzMPDQ',\n",
    "    'nl2CLw4PLFA',\n",
    "    'x0TheaRQ6ZE',\n",
    "    'gIxQOlxb2iY',\n",
    "    'GvxUyrWqmFg',\n",
    "    'olmhYxH8_HM'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7114b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos:  11%|███████▏                                                         | 1/9 [00:00<00:03,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for jV78iR4Vi68. 6 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  22%|██████████████▍                                                  | 2/9 [00:04<00:16,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for U62U2cgPagI. 268 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  33%|█████████████████████▋                                           | 3/9 [00:04<00:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for TB7yAjRgO-c. 287 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  44%|████████████████████████████▉                                    | 4/9 [00:05<00:05,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for BTUQ8dzMPDQ. 351 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  56%|████████████████████████████████████                             | 5/9 [00:05<00:03,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for nl2CLw4PLFA. 378 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  67%|███████████████████████████████████████████▎                     | 6/9 [00:11<00:08,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for x0TheaRQ6ZE. 819 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  78%|██████████████████████████████████████████████████▌              | 7/9 [00:12<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for gIxQOlxb2iY. 832 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing videos:  89%|█████████████████████████████████████████████████████████▊       | 8/9 [00:12<00:01,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for GvxUyrWqmFg. 865 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|█████████████████████████████████████████████████████████████████| 9/9 [00:15<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished fetching comments for olmhYxH8_HM. 1002 comments found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # get comments\n",
    "#     response = comment_threads(videoID='Pemat3Cp_NQ', to_csv=True)\n",
    "#     print(response)\n",
    "    for video_id in tqdm(video_ids, desc='Processing videos'):\n",
    "        response = comment_threads(videoID=video_id, to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cfc31ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nfrom dotenv import load_dotenv\\nfrom googleapiclient.discovery import build\\nfrom iteration_utilities import unique_everseen\\nimport csv\\ncomments = []\\ndef make_csv(comments, channelID=None):\\n    header = comments[0].keys()\\n\\n    if channelID:\\n#         filename = f\\'comments_{channelID}_{today}.csv\\'\\n        filename = f\\'comments.csv\\'\\n    else:\\n#         filename = f\\'comments_{today}.csv\\'\\n        filename = f\\'comments.csv\\'\\n\\n    with open(filename, \\'w\\', encoding=\\'utf8\\', newline=\\'\\') as f:\\n        writer = csv.DictWriter(f, fieldnames=header, extrasaction=\\'ignore\\')\\n        writer.writeheader()\\n        writer.writerows(comments)\\ndef comment_threads(videoID, to_csv=False):\\n    \\n    comments_list = []\\n    \\n    request = youtube.commentThreads().list(\\n        part=\\'id,replies,snippet\\',\\n        videoId=videoID,\\n    )\\n    response = request.execute()\\n    comments_list.extend(process_comments(response[\\'items\\']))\\n\\n    # if there is nextPageToken, then keep calling the API\\n    while response.get(\\'nextPageToken\\', None):\\n        request = youtube.commentThreads().list(\\n            part=\\'id,replies,snippet\\',\\n            videoId=videoID,\\n            pageToken=response[\\'nextPageToken\\']\\n        )\\n        response = request.execute()\\n        comments_list.extend(process_comments(response[\\'items\\']))\\n\\n    comments_list = list(unique_everseen(comments_list))\\n\\n    print(f\"Finished fetching comments for {videoID}. {len(comments_list)} comments found.\")\\n    \\n    if to_csv:\\n#         make_csv(comments_list, videoID)\\n        make_csv(comments_list)\\n    return comments_list\\nif __name__ == \\'__main__\\':\\n    # get comments\\n    response = comment_threads(videoID=\\'G4EVRUFttG0\\', to_csv=True)\\n    print(response)\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from iteration_utilities import unique_everseen\n",
    "import csv\n",
    "comments = []\n",
    "def make_csv(comments, channelID=None):\n",
    "    header = comments[0].keys()\n",
    "\n",
    "    if channelID:\n",
    "#         filename = f'comments_{channelID}_{today}.csv'\n",
    "        filename = f'comments.csv'\n",
    "    else:\n",
    "#         filename = f'comments_{today}.csv'\n",
    "        filename = f'comments.csv'\n",
    "\n",
    "    with open(filename, 'w', encoding='utf8', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(comments)\n",
    "def comment_threads(videoID, to_csv=False):\n",
    "    \n",
    "    comments_list = []\n",
    "    \n",
    "    request = youtube.commentThreads().list(\n",
    "        part='id,replies,snippet',\n",
    "        videoId=videoID,\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    # if there is nextPageToken, then keep calling the API\n",
    "    while response.get('nextPageToken', None):\n",
    "        request = youtube.commentThreads().list(\n",
    "            part='id,replies,snippet',\n",
    "            videoId=videoID,\n",
    "            pageToken=response['nextPageToken']\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_list.extend(process_comments(response['items']))\n",
    "\n",
    "    comments_list = list(unique_everseen(comments_list))\n",
    "\n",
    "    print(f\"Finished fetching comments for {videoID}. {len(comments_list)} comments found.\")\n",
    "    \n",
    "    if to_csv:\n",
    "#         make_csv(comments_list, videoID)\n",
    "        make_csv(comments_list)\n",
    "    return comments_list\n",
    "if __name__ == '__main__':\n",
    "    # get comments\n",
    "    response = comment_threads(videoID='G4EVRUFttG0', to_csv=True)\n",
    "    print(response)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
